{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've laid out the standard regression setting. Let's proceed to the Bayesian perspective.\n",
    "\n",
    "**Bayesian Neural Network (BNN)**\n",
    "\n",
    "Instead of point estimates for the parameters of $f$, a BNN places prior distributions over them.  Let's denote the weights and biases of the neural network by $w$.\n",
    "\n",
    "* **Prior:**  $p(w)$  (e.g.,  $w_j \\sim \\mathcal{N}(0, \\sigma_w^2)$ for each weight $w_j$)\n",
    "\n",
    "* **Likelihood:** $p(D | w)$. Assuming independent noise, this decomposes as:\n",
    "    $$p(D|w) = \\prod_{i=1}^n p(D_i|w) = \\prod_{i=1}^n p(f(x_i; w) + \\epsilon_i | w)$$\n",
    "    With your normal noise assumption:\n",
    "    $$p(D|w) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(D_i - f(x_i; w))^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "* **Posterior:** $p(w | D)$. This is our goal, representing the updated belief about the weights after observing the data. Bayes' theorem gives:\n",
    "    $$p(w | D) = \\frac{p(D|w) p(w)}{p(D)}$$\n",
    "    where $p(D) = \\int p(D|w) p(w) \\, dw$ is the marginal likelihood (evidence).\n",
    "\n",
    "Since the posterior is typically intractable for BNNs (due to the non-linearity of $f$), we resort to approximations.\n",
    "\n",
    "**Approximation Methods (briefly, as requested):**\n",
    "\n",
    "* **Variational Inference:** Approximate the posterior $p(w|D)$ with a simpler distribution $q(w|\\theta)$ parameterized by $\\theta$. Minimize the Kullback-Leibler divergence $KL[q(w|\\theta) || p(w|D)]$.\n",
    "\n",
    "* **Markov Chain Monte Carlo (MCMC):** Construct a Markov chain whose stationary distribution is the posterior $p(w|D)$. Sample from this chain to obtain samples representing the posterior.\n",
    "\n",
    "* **Laplace Approximation:**  Approximate the posterior with a Gaussian centered at the maximum a posteriori (MAP) estimate.\n",
    "\n",
    "This provides the core mathematical framework.  Further details on specific approximation methods can be provided if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's formalize this. You have a deterministic solver $S(h, D)$ that produces weights $w$ given hyperparameters $h$ and training data $D$. You also have a prior distribution over hyperparameters $p(h)$. Your neural network is a function $f(x; w)$ that maps inputs $x$ to outputs $y$ given weights $w$.\n",
    "\n",
    "You want to find $p(y | x, D)$, which can be written as:\n",
    "\n",
    "$$ p(y | x, D) = \\int p(y, h | x, D) \\, dh = \\int p(y | x, h, D) p(h | x, D) \\, dh $$\n",
    "\n",
    "Since $x$ is independent of $h$ given $D$ (the hyperparameters are chosen based on the data, not the specific input $x$), we have $p(h|x,D) = p(h|D)$.  Further, since $w = S(h,D)$ is deterministic, $y$ is deterministic given $x$, $h$ and $D$. We can write this as $y = f(x; S(h,D))$.  Therefore, $p(y | x, h, D)$ is a Dirac delta function:\n",
    "\n",
    "$$ p(y | x, h, D) = \\delta(y - f(x; S(h,D))) $$\n",
    "\n",
    "Substituting this into the integral, we get:\n",
    "\n",
    "$$ p(y | x, D) = \\int \\delta(y - f(x; S(h,D))) p(h|D) \\, dh $$\n",
    "\n",
    "Now, the tricky part is $p(h|D)$, the posterior distribution of hyperparameters given the data.  This is generally intractable to compute directly.  However, we can relate it to the prior $p(h)$ and the likelihood $p(D|h)$ using Bayes' theorem:\n",
    "\n",
    "$$ p(h|D) = \\frac{p(D|h) p(h)}{p(D)} $$\n",
    "\n",
    "where $p(D) = \\int p(D|h) p(h) \\, dh$ is the evidence.  The likelihood $p(D|h)$ is also complex to compute as it involves marginalizing over all possible weights $w$ that could be generated by the solver $S(h,D)$.\n",
    "\n",
    "Thus, the full expression becomes:\n",
    "\n",
    "$$ p(y | x, D) = \\frac{1}{p(D)} \\int \\delta(y - f(x; S(h,D))) p(D|h) p(h) \\, dh $$\n",
    "\n",
    "This integral is generally intractable.  A practical approach would be to approximate the posterior $p(h|D)$.  One way is to sample from $p(h)$ and then use importance sampling to re-weight the samples according to $p(D|h)$.  Alternatively, you could use variational inference to approximate $p(h|D)$ with a more tractable distribution.\n",
    "\n",
    "Another simplification, if you're just interested in sampling from $p(y | x, D)$, is to sample $h$ from $p(h)$, compute $w = S(h,D)$, and then compute $y = f(x; w)$.  This gives you samples from the joint distribution $p(y,h | x,D)$, and thus marginally from $p(y | x,D)$. This bypasses the need to compute $p(h|D)$ directly.  This is equivalent to implicitly using $p(h)$ as an approximation of $p(h|D)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down why sampling $h \\sim p(h)$ and then computing $y = f(x; S(h, D))$ gives you samples from $p(y, h | x, D)$, and consequently, $p(y | x, D)$.\n",
    "\n",
    "1. **Sampling from the Prior:** You start by sampling $h$ from your prior distribution $p(h)$.  This means the values of $h$ you obtain are representative of your initial beliefs about the hyperparameters, before considering the data.\n",
    "\n",
    "2. **Deterministic Computation of $w$ and $y$:**  Given a specific $h$ and the data $D$, the solver $S(h, D)$ deterministically produces the weights $w$. Subsequently, the neural network $f(x; w)$ deterministically produces the output $y$ for a given input $x$.\n",
    "\n",
    "3. **Joint Distribution:** Because $w$ and $y$ are deterministic functions of $h$ and $D$, sampling $h$ from $p(h)$ and then computing $y$ is equivalent to sampling from the joint distribution $p(y, h | x, D)$.  Think of it this way: for every $h$ drawn from $p(h)$, you're getting a corresponding $y$.  The frequency with which different $(h, y)$ pairs appear will be proportional to $p(h)$, and implicitly also reflects the complex relationship between $h$ and $y$ through $S(h,D)$ and $f(x; w)$.  We can also write it explicitly:\n",
    "   $p(y,h|x,D) = p(y|h,x,D) p(h|x,D)$. As discussed before, $p(h|x,D) = p(h|D)$, and with enough data, we might approximate $p(h|D) \\approx p(h)$. Also, $p(y|h,x,D) = \\delta(y-f(x;S(h,D)))$. Substituting this yields: $p(y,h|x,D) \\approx \\delta(y-f(x;S(h,D))) p(h)$. This shows, how by sampling $h$ from $p(h)$ and applying the deterministic functions $S$ and $f$ you are effectively sampling from an approximation to $p(y,h|x,D)$.\n",
    "\n",
    "\n",
    "4. **Marginal Distribution:**  If you have samples from a joint distribution $p(y, h | x, D)$, you automatically have samples from the marginal distribution $p(y | x, D)$.  To see why, consider a large number of $(h, y)$ samples.  If you ignore the $h$ values and only look at the $y$ values, these $y$ values will be distributed according to $p(y | x, D)$.  Formally, the marginal distribution is obtained by integrating the joint distribution over all possible values of $h$:\n",
    "    $$ p(y | x, D) = \\int p(y, h | x, D) \\, dh $$\n",
    "    By sampling from the joint distribution and then discarding the $h$ values, you are effectively performing this integration numerically.\n",
    "\n",
    "\n",
    "In summary, the procedure creates samples that reflect how likely different $y$ values are, given your prior beliefs about the hyperparameters and the deterministic mapping from hyperparameters to network outputs. This gives you a practical way to explore the distribution of possible outputs, even though calculating $p(y | x, D)$ directly is usually intractable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution to sample from p(y | x, D)\n",
    "- sample from $p(h | D)$ and compute $y=f(x; S(D,h))$\n",
    "- sample from $p(h | D)$ using importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance sampling is a technique for estimating properties of a distribution, such as expectations, when sampling directly from that distribution is difficult.  Here's how to apply it to sample from the posterior $p(h|D)$, and then ultimately the posterior predictive $p(y|x,D)$, recognizing the challenges:\n",
    "\n",
    "**1. Choose a Proposal Distribution $q(h)$:**\n",
    "\n",
    "You need a proposal distribution $q(h)$ that is easy to sample from and that has support everywhere $p(h|D)$ has support (meaning $q(h) > 0$ wherever $p(h|D) > 0$).  Common choices include:\n",
    "\n",
    "* **Gaussian distribution:**  Often centered around the maximum a posteriori (MAP) estimate of $h$.\n",
    "* **Uniform distribution:**  Over a region that contains most of the probability mass of $p(h|D)$.\n",
    "* **A more sophisticated distribution** tailored to your specific problem.\n",
    "\n",
    "**2. Calculate the Importance Weights:**\n",
    "\n",
    "For each sample $h_i$ drawn from $q(h)$, calculate the importance weight:\n",
    "\n",
    "$$ w_i = \\frac{p(h_i|D)}{q(h_i)} = \\frac{p(D|h_i)p(h_i)}{p(D)q(h_i)} \\propto \\frac{p(D|h_i)p(h_i)}{q(h_i)} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $p(D|h_i)$ is the likelihood of the data given the parameters $h_i$.\n",
    "* $p(h_i)$ is the prior distribution over the parameters.\n",
    "* $p(D)$ is the evidence (marginal likelihood), which is often difficult to compute.  However, since it's a constant with respect to $h$, we can often ignore it and work with unnormalized weights.  We'll then normalize the weights later.\n",
    "\n",
    "**3. Normalize the Weights:**\n",
    "\n",
    "$$ \\tilde{w}_i = \\frac{w_i}{\\sum_{j=1}^N w_j} $$\n",
    "\n",
    "where $N$ is the number of samples.\n",
    "\n",
    "**4.  Approximate Expectations:**\n",
    "\n",
    "You can now approximate expectations of functions of $h$ with respect to the posterior:\n",
    "\n",
    "$$ E_{p(h|D)}[g(h)] \\approx \\sum_{i=1}^N \\tilde{w}_i g(h_i) $$\n",
    "\n",
    "**5. Sampling from the Posterior Predictive:**\n",
    "\n",
    "To get samples from $p(y|x,D) = \\int \\delta(y - f(x;S(h,D)))p(h|D) dh$:\n",
    "1. Draw $h_i$ from the proposal distribution $q(h)$.\n",
    "2. Compute $w_i$ and $\\tilde{w}_i$ as described in steps 2 and 3.\n",
    "3. Sample from $p(y | x, h_i)$ (if $S(h,D)=h$), where the sample is weighted by $\\tilde{w}_i$. Since we are using a delta function here, we just compute $y_i = f(x; S(h_i, D))$ and consider it a sample of the posterior predictive *weighted by $\\tilde{w}_i$*.\n",
    "\n",
    "In practice, $y_i$ could be easily obtained since we are usually working with deterministic models.\n",
    "\n",
    "If $f$ is stochastic, we could approximate the expectation over $y$ given $x$ by: $E_{p(y|x,D)} \\approx \\sum_{i=1}^N \\tilde{w}_i f(x;S(h_i,D))$\n",
    "\n",
    "**Challenges with Importance Sampling for Posterior Distributions:**\n",
    "\n",
    "* **Curse of Dimensionality:** As the dimensionality of $h$ increases, it becomes increasingly difficult to find a good proposal distribution $q(h)$ that adequately covers the important regions of $p(h|D)$. This can lead to very large weights for a small proportion of the samples, negating the benefit.\n",
    "* **Choosing a Good Proposal Distribution:**  The effectiveness of importance sampling hinges critically on the choice of $q(h)$.  A poor choice can lead to highly variable estimates and slow convergence.\n",
    "\n",
    "**Alternatives and Improvements:**\n",
    "\n",
    "If importance sampling proves problematic, consider these alternatives:\n",
    "\n",
    "* **Markov Chain Monte Carlo (MCMC):**  MCMC methods are often preferred for high-dimensional posteriors.\n",
    "* **Sequential Monte Carlo (SMC):**  SMC methods can be more robust than standard importance sampling, especially in complex scenarios.\n",
    "* **Adaptive Importance Sampling:**  These methods try to iteratively improve the proposal distribution during the sampling process.\n",
    "\n",
    "\n",
    "If you can provide more details about your specific problem (e.g., the form of your likelihood and prior), I can give more specific advice on choosing a suitable approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute $p(y(x)|D)$, where $y(x) = \\hat{f}(x, W)$ and $W = s(H, D)$.\n",
    "Since $W$ is deterministic given $H$ and $D$, we can write $y(x) = \\hat{f}(x, s(H, D))$.\n",
    "We want to find the probability distribution of $y(x)$ given the data $D$.  This can be written as:\n",
    "\n",
    "$p(y(x)|D) = \\int p(y(x)|H, D) p(H|D) dH$\n",
    "\n",
    "Since $y(x)$ is deterministic given $H$ and $D$, $p(y(x)|H, D)$ is a Dirac delta function:\n",
    "\n",
    "$p(y(x)|H, D) = \\delta(y(x) - \\hat{f}(x, s(H, D)))$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$p(y(x)|D) = \\int \\delta(y(x) - \\hat{f}(x, s(H, D))) p(H|D) dH$\n",
    "\n",
    "By Bayes' theorem, we have:\n",
    "\n",
    "$p(H|D) = \\frac{p(D|H)p(H)}{p(D)}$\n",
    "\n",
    "So,\n",
    "\n",
    "$p(y(x)|D) = \\int \\delta(y(x) - \\hat{f}(x, s(H, D))) \\frac{p(D|H)p(H)}{p(D)} dH$\n",
    "\n",
    "Since $D_i = f(x_i) + E$ and $E \\sim \\mathcal{N}(0, \\sigma^2)$, we can write the likelihood function as:\n",
    "\n",
    "$p(D|H) = \\prod_{i=1}^m \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(D_i - f(x_i))^2}{2\\sigma^2}\\right)$\n",
    "\n",
    "Note that this likelihood does not depend on $H$ directly, but only through the choice of $f(x_i)$, which we are trying to approximate with $\\hat{f}(x_i, W)$.  Ideally, we'd want to replace $f(x_i)$ with $\\hat{f}(x_i, s(H,D))$ in the likelihood, but this creates a complicated dependency.  If we assume $\\hat{f}$ is a good approximation to $f$, the likelihood will be close to a constant, independent of $H$.\n",
    "\n",
    "$p(D) = \\int p(D|H)p(H) dH$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$p(y(x)|D) = \\frac{1}{p(D)} \\int \\delta(y(x) - \\hat{f}(x, s(H, D))) p(D|H) p(H) dH$\n",
    "\n",
    "\n",
    "This integral is difficult to evaluate analytically in general.  We can approximate it using Monte Carlo methods:  draw samples $H_j$ from $p(H)$ and approximate the integral as:\n",
    "\n",
    "$p(y(x)|D) \\approx \\frac{1}{N p(D)} \\sum_{j=1}^N \\delta(y(x) - \\hat{f}(x, s(H_j, D))) p(D|H_j)$\n",
    "\n",
    "\n",
    "Final Answer: The final answer is $\\boxed{p(y(x)|D) = \\int \\delta(y(x) - \\hat{f}(x, s(H, D))) p(H|D) dH}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're right to question that. My wording was imprecise and led to the confusion. The statement \"this likelihood does not depend on H directly\" is misleading.\n",
    "\n",
    "The likelihood $p(D|H)$ does depend on $H$ indirectly if our model $\\hat{f}$ depends on $H$, because $H$ influences what $\\hat{f}$ we choose, which in turn influences how well $\\hat{f}$ predicts the noisy observations $D$. A better way to express the relationship is to say that given a fixed underlying function f(x), the likelihood $p(D|H)$ does not depend on $H$. That's why I mentioned that ideally, $f(x_i)$ would be replaced with $\\hat{f}(x_i, s(H,D))$. The problem is that it's hard to make this substitution directly in the likelihood because it creates a circular dependency.\n",
    "\n",
    "Here's a clearer breakdown:\n",
    "\n",
    "H influences the choice of the model $\\hat{f}$: $H$ are the hyperparameters that $s$ uses to select a specific function $\\hat{f}$ from a family of functions.\n",
    "$\\hat{f}$ attempts to model the underlying function $f$: $\\hat{f}(x, s(H,D))$ is our approximation of the true function $f(x)$ that generated the data.\n",
    "The likelihood $p(D|H)$ reflects how well $\\hat{f}$ explains the observed data $D$: A good choice of $H$ will lead to a $\\hat{f}$ that accurately predicts $D$, resulting in a higher likelihood.\n",
    "So, while $H$ doesn't appear directly in the expression for $p(D|H)$ when we write it with $f(x_i)$, the choice of $H$ does affect $p(D|H)$ indirectly through its influence on $\\hat{f}$. This indirect dependence is crucial and is why we can't simply ignore $H$ in the likelihood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're absolutely right. The core issue is that we're trying to use a likelihood based on the true function *f*, which we *don't know*. We're trying to *learn* f.  Using $f$ in the likelihood $p(D|H)$ is conceptually incorrect when our goal is to infer properties of $\\hat{f}$, our learned approximation of $f$.\n",
    "\n",
    "The initial formulation $p(y(x)|D)$ itself is correct.  It represents what we want: the posterior predictive distribution. It tells us the probability of observing a new value $y(x)$ at a new input $x$, given the observed data $D$.\n",
    "\n",
    "The problem arises when we try to decompose this posterior predictive using the hyperparameters $H$.  The typical Bayesian approach involves a likelihood $p(D|\\theta)$, where $\\theta$ are the parameters of our model.  In our case, our \"parameters\" are really $W = s(H, D)$, which are deterministically derived from $H$ and $D$.  And our model isn't directly parameterized by $f$.\n",
    "\n",
    "Here's a more accurate approach, highlighting the problem and a possible solution:\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "We want to write something like $p(D|H)$, but $D_i = f(x_i) + \\epsilon_i$.  We *don't know* $f$.  Substituting $\\hat{f}(x_i, s(H, D))$ directly for $f(x_i)$ in the likelihood creates a circular dependency: $D$ appears on both sides, and the likelihood becomes a function of the data itself, which isn't what we want.\n",
    "\n",
    "**A More Accurate Approach (Approximate):**\n",
    "\n",
    "1. **Treat $\\hat{f}$ as the generating process:**  Instead of conditioning on the unknown $f$, we condition on our learned model $\\hat{f}$.  This is an approximation, but a necessary one.  Our likelihood becomes:\n",
    "\n",
    "   $p(D|H) \\approx \\prod_{i=1}^m \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(D_i - \\hat{f}(x_i, s(H,D)))^2}{2\\sigma^2}\\right)$\n",
    "\n",
    "2. **Acknowledge the approximation:**  This likelihood is now an approximation. It represents how well the *learned* model $\\hat{f}$, determined by $H$ and $D$, explains the data $D$.\n",
    "\n",
    "3. **Proceed with the posterior predictive:**  Now we can use this approximate likelihood in our posterior predictive calculation:\n",
    "\n",
    "   $p(y(x)|D) \\approx \\int \\delta(y(x) - \\hat{f}(x, s(H, D))) \\frac{p(D|H)p(H)}{p(D)} dH$\n",
    "\n",
    "   Where $p(D|H)$ is the approximation from step 1.\n",
    "\n",
    "\n",
    "This approach makes the approximation explicit.  It replaces the true, unknown $f$ with our learned $\\hat{f}$ in the likelihood. While not perfectly Bayesian, it's a practical way to compute the posterior predictive distribution. The key is to be aware of the approximation being made.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate Bayesian Computation (ABC) is a family of methods used for Bayesian inference when the likelihood function $p(D|\\theta)$ is intractable or too expensive to compute.  Instead of directly evaluating the likelihood, ABC methods rely on simulations and comparisons between simulated and observed data.  Here's a breakdown:\n",
    "\n",
    "**Core Idea:**\n",
    "\n",
    "If we can simulate data from our model, and the simulated data looks very similar to our observed data, then the parameters used for that simulation are likely to be plausible values for the true parameters.\n",
    "\n",
    "**Basic Algorithm (Rejection ABC):**\n",
    "\n",
    "1. **Prior Sampling:** Draw a parameter value $\\theta^*$ from the prior distribution $p(\\theta)$.\n",
    "2. **Simulation:** Generate a simulated dataset $D^*$ from the model $p(D|\\theta^*)$.\n",
    "3. **Comparison:** Calculate a distance metric $\\rho(D^*, D)$ between the simulated data $D^*$ and the observed data $D$.\n",
    "4. **Acceptance/Rejection:**  If $\\rho(D^*, D) \\le \\epsilon$, where $\\epsilon$ is a tolerance threshold, accept $\\theta^*$. Otherwise, reject $\\theta^*$.\n",
    "5. **Repeat:** Repeat steps 1-4 many times.  The accepted $\\theta^*$ values form an approximate sample from the posterior distribution $p(\\theta|D)$.\n",
    "\n",
    "**Key Components and Considerations:**\n",
    "\n",
    "* **Summary Statistics:** Often, comparing entire datasets is computationally expensive or not meaningful. Instead, we use summary statistics $S(D)$ and $S(D^*)$, reducing the dimensionality and focusing on relevant features. The distance metric then becomes $\\rho(S(D^*), S(D))$.\n",
    "* **Distance Metric:** The choice of distance metric $\\rho$ is crucial. It should reflect the important differences between datasets relevant to the inference problem. Common choices include Euclidean distance, Mahalanobis distance, and other dissimilarity measures.\n",
    "* **Tolerance Threshold ($\\epsilon$):** The tolerance $\\epsilon$ controls the trade-off between computational cost and accuracy.  Smaller $\\epsilon$ leads to a more accurate approximation of the posterior but requires more simulations.\n",
    "* **Prior Distribution ($p(\\theta)$):** As in standard Bayesian inference, the prior distribution plays a role in shaping the posterior approximation.\n",
    "\n",
    "**More Advanced ABC Algorithms:**\n",
    "\n",
    "* **Markov Chain Monte Carlo ABC (MCMC-ABC):**  Uses MCMC to explore the parameter space more efficiently than rejection ABC.\n",
    "* **Sequential Monte Carlo ABC (SMC-ABC):**  Employs a sequence of decreasing tolerances $\\epsilon$ to gradually refine the posterior approximation.\n",
    "\n",
    "**Advantages of ABC:**\n",
    "\n",
    "* **Handles intractable likelihoods:**  The primary advantage is its applicability when the likelihood is difficult or impossible to compute.\n",
    "* **Intuitive and easy to implement:**  The basic algorithm is relatively straightforward to understand and code.\n",
    "\n",
    "**Disadvantages of ABC:**\n",
    "\n",
    "* **Choice of summary statistics:** Selecting informative summary statistics can be challenging and requires domain expertise.  Poorly chosen statistics can lead to inaccurate inferences.\n",
    "* **Tolerance selection:** Choosing an appropriate tolerance $\\epsilon$ is crucial but often requires trial and error.\n",
    "* **Computational cost:**  ABC can be computationally intensive, particularly for complex models or small tolerances.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Imagine inferring the mutation rate in a population based on genetic data. The likelihood function for this model might be incredibly complex.  Using ABC, we can simulate genetic data under different mutation rates, calculate summary statistics (e.g., allele frequencies), and compare them to the observed data to approximate the posterior distribution of the mutation rate.\n",
    "\n",
    "In the context of your original question, ABC could be used to approximate $p(H|D)$ without needing to evaluate the problematic likelihood $p(D|H)$ directly.  You would simulate datasets $D^*$ using different $H$ values, compute summary statistics on $D^*$ and $D$, and accept $H$ values where the simulated summary statistics are close to the observed summary statistics.  This provides a way to sample from the approximate posterior distribution of the hyperparameters $H$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\vec{D} = (D_i)_{i=1}^{n}$ with $D_i = f(x_i) + E_i$ with $E_i \\sim \\mathcal{N}(0, \\sigma^2)$, where $f$ is unknown.\n",
    "\n",
    "Let $d$ be a realization of $D$ and $s$ be a deterministic solver that returns the parameters of a function $\\hat{f}$ that approximates $f$:\n",
    "\n",
    "$$\n",
    "s(d) = \\argmin_{\\theta} \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\hat{f}(x_i, \\theta) - d_i \\right)^2.\n",
    "$$\n",
    "\n",
    "Let $g$ be a function on a set of datapoints (e.g. a function that generates a synthetic dataset from a given dataset). $g$ also takes another parameter.\n",
    "\n",
    "$$\n",
    "g(d, q) = \\tilde{d}\n",
    "$$\n",
    "\n",
    "Let $Q$ be a random variable with distribution $p(Q)$.\n",
    "\n",
    "Let $\\hat{Y}(x) = \\hat{f}(x, s(g(D, Q)))$.\n",
    "\n",
    "I already obtained samples $\\hat{Y}(x)$ by simply sampling $q$'s from $p(Q)$ and now want to use importance sampling to obtain:\n",
    "\n",
    "$$\n",
    "p(\\hat{Y}(x) | D)\n",
    "$$\n",
    "\n",
    "No further assumptions can be made. Attempt the problem without any yapping about its diffculty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to estimate $p(\\hat{Y}(x) | D)$.  Since we have samples of $\\hat{Y}(x)$ generated using $p(Q)$, we can rewrite the desired distribution using importance sampling.\n",
    "\n",
    "Let $\\hat{Y}_q(x) = \\hat{f}(x, s(g(D, q)))$ be the output for a specific value of $q$.  Then,\n",
    "\n",
    "$$ p(\\hat{Y}(x) | D) = \\int p(\\hat{Y}(x) | Q, D) p(Q | D) dq $$\n",
    "\n",
    "We can approximate this with importance sampling using the samples generated with $p(Q)$. The importance weights are given by:\n",
    "\n",
    "$$ w(q) = \\frac{p(Q=q | D)}{p(Q=q)} $$\n",
    "\n",
    "However, $p(Q|D)$ is generally unknown and difficult to calculate.  We can, however, rewrite the integral in terms of $q$:\n",
    "\n",
    "$$ p(\\hat{Y}(x) = y | D) = \\int \\delta(y - \\hat{Y}_q(x)) p(Q=q | D) dq $$\n",
    "\n",
    "Using importance sampling with proposal distribution $p(Q)$, we get:\n",
    "\n",
    "$$ p(\\hat{Y}(x) = y | D) \\approx \\frac{1}{N} \\sum_{i=1}^N \\delta(y - \\hat{Y}_{q_i}(x)) w(q_i) $$\n",
    "where $q_i \\sim p(Q)$ and $N$ is the number of samples.\n",
    "\n",
    "Since $w(q_i) = \\frac{p(Q=q_i | D)}{p(Q=q_i)}$, and we don't know $p(Q|D)$, we need to approximate it.  Without further assumptions, the best we can do is to use a potentially crude approximation.\n",
    "\n",
    "One option, assuming $Q$ doesn't depend strongly on $D$ (which may not be true), is to approximate $p(Q|D) \\approx p(Q)$. This leads to $w(q_i) \\approx 1$.  This effectively ignores the importance sampling and treats the samples as if they were drawn from the correct distribution.  This gives:\n",
    "\n",
    "$$ p(\\hat{Y}(x) = y | D) \\approx \\frac{1}{N} \\sum_{i=1}^N \\delta(y - \\hat{Y}_{q_i}(x)) $$\n",
    "\n",
    "This is equivalent to constructing a histogram of the $\\hat{Y}_{q_i}(x)$ samples.  This is a crude approximation, but without further assumptions about the relationship between $Q$ and $D$, it's difficult to do better.\n",
    "\n",
    "Alternatively, if you have a model for the relationship between $D$ and $Q$, even a very approximate one, you could use it to estimate $p(Q|D)$ and obtain more accurate importance weights.  For example, if you can assume a parametric form for $p(Q|D)$, you could try to learn its parameters.\n",
    "\n",
    "\n",
    "Final Answer: The final answer is $\\boxed{p(\\hat{Y}(x) = y | D) \\approx \\frac{1}{N} \\sum_{i=1}^N \\delta(y - \\hat{Y}_{q_i}(x))}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\vec{D} = (D_i)_{i=1}^{n}$ with $D_i = f(x_i) + E_i$ with $E_i \\sim \\mathcal{N}(0, \\sigma^2)$, where $f$ is unknown.\n",
    "\n",
    "Let $d$ be a realization of $D$ and $s$ be a deterministic solver that returns the parameters of a function $\\hat{f}$ that approximates $f$:\n",
    "\n",
    "$$\n",
    "s(d) = \\argmin_{\\theta} \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\hat{f}(x_i, \\theta) - d_i \\right)^2.\n",
    "$$\n",
    "\n",
    "Let $g$ be a function on a set of datapoints (e.g. a function that generates a synthetic dataset from a given dataset). $g$ also takes another parameter.\n",
    "\n",
    "$$\n",
    "g(d, q) = \\tilde{d}\n",
    "$$\n",
    "\n",
    "Let $Q$ be a random variable with distribution $p(Q)$.\n",
    "\n",
    "I already obtained samples $\\hat{f}(x, s(g(D, q_k)))$ by simply sampling $q_k$'s from $p(Q)$ and now want to use importance sampling to obtain:\n",
    "\n",
    "$$\n",
    "p(\\hat{f}(x, s(g(D, Q))) | D)\n",
    "$$\n",
    "\n",
    "No further assumptions can be made. Attempt the problem without any yapping about its diffculty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's denote $\\hat{f}(x, s(g(d,q)))$ as $h(d, q)$ for brevity.  We want to estimate $p(h(D,Q)|D=d)$.  Since we have samples from $h(d, q_k)$ where $q_k \\sim p(Q)$, we can use importance sampling.  The target distribution is $p(h(D,Q)|D=d)$, but we don't have direct access to it.  We can rewrite the target using the law of total probability:\n",
    "\n",
    "$$p(h(D,Q)|D=d) = \\int p(h(D,Q)|Q=q, D=d) p(Q=q|D=d) dq$$\n",
    "\n",
    "We can approximate this using the samples $q_k$ drawn from $p(Q)$:\n",
    "\n",
    "$$p(h(D,Q)|D=d) \\approx \\frac{1}{K} \\sum_{k=1}^K p(h(D,Q)|Q=q_k, D=d) \\frac{p(Q=q_k|D=d)}{p(Q=q_k)}$$\n",
    "\n",
    "Since $Q$ is independent of $D$, $p(Q=q_k|D=d) = p(Q=q_k)$, simplifying the above to:\n",
    "\n",
    "$$p(h(D,Q)|D=d) \\approx \\frac{1}{K} \\sum_{k=1}^K p(h(D,Q)|Q=q_k, D=d)$$\n",
    "\n",
    "Now, $h(D,Q)$ is deterministic given $D$ and $Q$, so $p(h(D,Q)|Q=q_k, D=d)$ is a Dirac delta function centered at $h(d, q_k)$. This doesn't lend itself to practical computation.  \n",
    "\n",
    "Instead, we can consider approximating the distribution of $h(D,Q)|D=d$. We can do this by constructing a kernel density estimate (KDE) using the samples $h(d, q_k)$.  Let $K_h(\\cdot)$ be a kernel function with bandwidth $h$:\n",
    "\n",
    "$$\\hat{p}(h(D,Q) = y | D=d) = \\frac{1}{K} \\sum_{k=1}^{K} K_h(y - h(d, q_k))$$\n",
    "\n",
    "\n",
    "This KDE provides an approximation of the desired conditional distribution using the available samples.  Note that the choice of kernel and bandwidth will affect the quality of the approximation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is my procedure to approximate the uncertainty of the output of a NN conditioned on the training data $d$:\n",
    "1. generate samples $u_k$, $k=1 \\dots m$ ($u \\in [0,1]$)\n",
    "2. generate synthetic datasets $d_k \\coloneqq g(d, u_k)$\n",
    "3. train NNs $\\hat{f}_k$ on those\n",
    "4. evaluate those: $\\hat{y}_k(x) = \\hat{f}_k(x)$\n",
    "5. weight samples such that correct distribution is obtained\n",
    "\n",
    "I am not sure about the weighting process, which is why I come to you. I just know that I have to reweight because of the following thought experiment:\n",
    "I know that $g$ gives synthetic datasets that can look completely different from another for large, but different $u$'s. The opposite is true for small but different $u$'s.\n",
    "Imagine two different sampling strategies for $u_k$. The first gives very large $u$'s and the second very small ones. Since my NN learns these datasets, the same while hold for the predicted values $\\hat{y}_k(x)$: First samling strategy -> high variance. Second samling strategy -> low variance. So two different uncertainty quantifications for the same dataset $d$.\n",
    "\n",
    "Formalize my procedure step by step using exact mathematical notation.\n",
    "\n",
    "But before doing so, formalize the procedure of using Bayes neural networks (BNNs) with MCMC to approximate the uncertainty of the output of a NN conditioned on the training data. Include every step in the process, as if this intended for a reader hearing the first time about BNNs.\n",
    "\n",
    "Make sure to use consistent notation: Uppercase for random variables, f,g,h,... for functions and so on. Be extra precise and pedantic about everything.\n",
    "\n",
    "When you are done with BNNs, let me read over it and then we may proceed with my procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's formalize Bayesian Neural Networks and your proposed procedure for uncertainty quantification.\n",
    "\n",
    "**Bayesian Neural Networks (BNNs) with MCMC for Uncertainty Quantification**\n",
    "\n",
    "1. **Model Definition:** A neural network can be viewed as a function $f(x; w)$, where $x \\in \\mathcal{X}$ is the input, $w \\in \\mathcal{W}$ are the network weights, and $f: \\mathcal{X} \\times \\mathcal{W} \\to \\mathcal{Y}$ maps inputs to outputs in the space $\\mathcal{Y}$.  In a Bayesian setting, we treat the weights $W$ as a random variable.\n",
    "\n",
    "2. **Prior Distribution:** We define a prior distribution $p(W)$ over the weights. This prior represents our initial beliefs about the weights before observing any data.  Common choices include Gaussian priors, e.g., $W \\sim \\mathcal{N}(0, \\sigma^2 I)$, where $I$ is the identity matrix.\n",
    "\n",
    "3. **Likelihood Function:** Given a dataset $d = \\{(x_i, y_i)\\}_{i=1}^n$ consisting of $n$ input-output pairs, we define a likelihood function $p(D|W)$, which represents the probability of observing the data given the weights.  Assuming independent and identically distributed (i.i.d.) data points and a Gaussian noise model, the likelihood can be written as:\n",
    "\n",
    "   $$p(D|W) = \\prod_{i=1}^n p(y_i|x_i, W) = \\prod_{i=1}^n \\mathcal{N}(y_i | f(x_i; W), \\sigma_n^2)$$\n",
    "\n",
    "   where $\\sigma_n^2$ is the noise variance.\n",
    "\n",
    "4. **Posterior Distribution:** Using Bayes' theorem, we can compute the posterior distribution $p(W|D)$, which represents our updated beliefs about the weights after observing the data:\n",
    "\n",
    "   $$p(W|D) = \\frac{p(D|W) p(W)}{p(D)}$$\n",
    "\n",
    "   where $p(D) = \\int p(D|W) p(W) dW$ is the marginal likelihood (evidence), which acts as a normalizing constant.\n",
    "\n",
    "5. **Markov Chain Monte Carlo (MCMC) Sampling:** Since the posterior distribution is often intractable to compute directly, we use MCMC methods to draw samples from it.  A common MCMC method is the Metropolis-Hastings algorithm.  It constructs a Markov chain whose stationary distribution is the target posterior distribution $p(W|D)$.  The algorithm proceeds as follows:\n",
    "\n",
    "    a. Initialize the weights $w^{(0)}$.\n",
    "    b. For $t = 1, 2, \\dots, T$:\n",
    "    i. Propose a new set of weights $w'$ from a proposal distribution $q(w'|w^{(t-1)})$.\n",
    "    ii. Calculate the acceptance ratio:\n",
    "\n",
    "    $$\\alpha = \\min\\left(1, \\frac{p(D|w')p(w')q(w^{(t-1)}|w')}{p(D|w^{(t-1)})p(w^{(t-1)})q(w'|w^{(t-1)})}\\right)$$\n",
    "\n",
    "    iii. Accept the proposed weights $w'$ with probability $\\alpha$, i.e., set $w^{(t)} = w'$. Otherwise, set $w^{(t)} = w^{(t-1)}$.\n",
    "\n",
    "6. **Predictive Distribution:**  After obtaining $T$ samples $\\{w^{(t)}\\}_{t=1}^T$ from the posterior, we can approximate the predictive distribution for a new input $x^*$:\n",
    "\n",
    "   $$p(Y^*|x^*, D) \\approx \\frac{1}{T} \\sum_{t=1}^T p(Y^*|x^*, w^{(t)}) = \\frac{1}{T} \\sum_{t=1}^T \\mathcal{N}(Y^* | f(x^*; w^{(t)}), \\sigma_n^2)$$\n",
    "\n",
    "   This predictive distribution provides both a point estimate (e.g., the mean) and uncertainty quantification (e.g., the variance or credible intervals).\n",
    "\n",
    "Now, let's proceed with your procedure.  Please let me know when you're ready.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Predictive Distribution:** The goal is to predict the output $Y^*$ for a new input $x^*$ given the observed dataset $D$. In the Bayesian framework, this involves computing the predictive distribution $p(Y^*|x^*, D)$. This distribution incorporates the uncertainty over the weights $W$ by marginalizing over the posterior distribution $p(W|D)$:\n",
    "\n",
    "   $$p(Y^*|x^*, D) = \\int p(Y^*|x^*, W) p(W|D) dW$$\n",
    "\n",
    "   Here, $p(Y^*|x^*, W)$ represents the likelihood of observing the output $Y^*$ given the input $x^*$ and a specific set of weights $W$.  As before, we typically assume a Gaussian noise model:\n",
    "\n",
    "   $$p(Y^*|x^*, W) = \\mathcal{N}(Y^* | f(x^*; W), \\sigma_n^2)$$\n",
    "\n",
    "   Since the integral in the predictive distribution is often intractable, we approximate it using the MCMC samples obtained in the previous step.  Specifically, we use the Monte Carlo estimator:\n",
    "\n",
    "   $$p(Y^*|x^*, D) \\approx \\frac{1}{T} \\sum_{t=1}^T p(Y^*|x^*, w^{(t)})$$\n",
    "\n",
    "   where $\\{w^{(t)}\\}_{t=1}^T$ are the $T$ samples drawn from the posterior distribution $p(W|D)$ using MCMC. Substituting the Gaussian likelihood, we get:\n",
    "\n",
    "   $$p(Y^*|x^*, D) \\approx \\frac{1}{T} \\sum_{t=1}^T \\mathcal{N}(Y^* | f(x^*; w^{(t)}), \\sigma_n^2)$$\n",
    "\n",
    "   This approximation represents a mixture of Gaussian distributions, each centered around the network's prediction $f(x^*; w^{(t)})$ with variance $\\sigma_n^2$ for each sampled weight vector $w^{(t)}$.\n",
    "\n",
    "   From this approximated predictive distribution, we can extract various quantities for uncertainty quantification:\n",
    "\n",
    "   * **Mean Prediction:**  The mean of the predictive distribution provides a point estimate for $Y^*$:\n",
    "     $$E[Y^*|x^*, D] \\approx \\frac{1}{T} \\sum_{t=1}^T f(x^*; w^{(t)})$$\n",
    "\n",
    "   * **Variance:** The variance of the predictive distribution quantifies the uncertainty in the prediction:\n",
    "     $$Var[Y^*|x^*, D] \\approx \\frac{1}{T} \\sum_{t=1}^T \\left[ (f(x^*; w^{(t)}) - E[Y^*|x^*, D])^2 + \\sigma_n^2 \\right]$$\n",
    "\n",
    "   * **Credible Intervals:**  A $(1-\\alpha)\\%$ credible interval represents an interval within which the true value of $Y^*$ lies with $(1-\\alpha)\\%$ probability.  These intervals can be estimated from the quantiles of the samples $\\{f(x^*; w^{(t)})\\}_{t=1}^T$.  For example, a 95% credible interval is defined by the 2.5th and 97.5th percentiles of the samples.\n",
    "\n",
    "\n",
    "This detailed explanation clarifies how the predictive distribution is approximated using MCMC samples and how various uncertainty measures can be derived.  Now we can move on to your proposed procedure when you're ready.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's formalize your procedure for approximating the uncertainty of a neural network's output conditioned on the training data.\n",
    "\n",
    "**Your Proposed Procedure**\n",
    "\n",
    "1. **Generate Samples:** Generate $m$ independent and identically distributed (i.i.d.) samples $U_k$ from a uniform distribution over the unit interval, i.e., $U_k \\sim \\mathcal{U}(0, 1)$ for $k = 1, \\dots, m$.\n",
    "\n",
    "2. **Generate Synthetic Datasets:**  Apply a function $g$ to the original dataset $d$ and each sample $U_k$ to create $m$ synthetic datasets $d_k$:\n",
    "\n",
    "   $$d_k = g(d, U_k), \\quad k = 1, \\dots, m$$\n",
    "\n",
    "   The function $g: \\mathcal{D} \\times [0, 1] \\to \\mathcal{D}$ maps a dataset and a uniform random variable to a new synthetic dataset.  The exact form of $g$ will determine how the synthetic datasets are generated.  This is where your intuition about small $u$ values leading to similar datasets and large $u$ values leading to dissimilar datasets comes into play.\n",
    "\n",
    "3. **Train Neural Networks:**  Train $m$ neural networks $\\hat{f}_k(x; w_k)$, where $w_k$ are the weights of the $k$-th network, on each of the synthetic datasets $d_k$.  This results in $m$ different trained networks:\n",
    "\n",
    "   $$\\hat{f}_k = \\text{train}(d_k), \\quad k = 1, \\dots, m$$\n",
    "\n",
    "   Here, $\\text{train}(\\cdot)$ represents the training process, which could be any standard training algorithm (e.g., stochastic gradient descent).\n",
    "\n",
    "4. **Evaluate Networks:** For a new input $x$, evaluate each of the $m$ trained networks to obtain $m$ predicted outputs:\n",
    "\n",
    "   $$\\hat{y}_k(x) = \\hat{f}_k(x; w_k), \\quad k = 1, \\dots, m$$\n",
    "\n",
    "5. **Weight Samples:**  Compute weights $\\omega_k$ for each prediction $\\hat{y}_k(x)$ such that the weighted average of the predictions approximates the true predictive distribution conditioned on the original dataset $d$. This is the crucial step where we need to determine the appropriate weighting scheme.\n",
    "\n",
    "   The goal is to find weights $\\omega_k(U_k)$ such that:\n",
    "\n",
    "   $$ p(Y|x, D) \\approx \\sum_{k=1}^m \\omega_k(U_k) \\delta(Y - \\hat{y}_k(x)) $$\n",
    "\n",
    "   where $\\delta(\\cdot)$ is the Dirac delta function.\n",
    "\n",
    "To determine $\\omega_k(U_k)$, we have to compute $p(Y|x, D)$ by marginalization. Would you like me to perform this calculation now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to approximate $p(Y|x, D)$, the true predictive distribution given the original dataset $D$.  Your method generates synthetic datasets $d_k = g(D, U_k)$ using the transformation function $g$ and random variables $U_k$.  The key is to understand how the distribution of the synthetic datasets relates to the true data distribution.\n",
    "\n",
    "Let's denote the true data distribution as $p(D)$. We can think of your process as generating samples from a distribution $p_g(D')$ of datasets $D'$, induced by $g$ and the distribution of $U$.  Formally, this is given by:\n",
    "\n",
    "$$p_g(D') = \\int p(U) \\delta(D' - g(D, U)) dU$$\n",
    "\n",
    "where $\\delta$ is the Dirac delta function.  Ideally, we would like $p_g(D')$ to be close to $p(D)$.  However, this is generally not the case, especially if $g$ introduces significant distortions.\n",
    "\n",
    "Now, consider the predictive distribution given a synthetic dataset $d_k$:\n",
    "\n",
    "$$p(Y|x, d_k) \\approx \\mathcal{N}(Y | \\hat{f}_k(x), \\sigma_{n,k}^2)$$\n",
    "\n",
    "where we're approximating the predictive distribution with a Gaussian centered at the network prediction $\\hat{f}_k(x)$ with a variance $\\sigma_{n,k}^2$ (which could be estimated during training).\n",
    "\n",
    "To connect this back to the true predictive distribution, we can think of marginalizing over the distribution of synthetic datasets:\n",
    "\n",
    "$$p(Y|x, D) = \\int p(Y|x, D') p(D'|D) dD'$$\n",
    "\n",
    "This is where the weighting comes in.  We want to approximate this integral using our samples $d_k$.  The weights $\\omega_k(U_k)$ should ideally be proportional to the importance weights:\n",
    "\n",
    "$$\\omega_k(U_k) \\propto \\frac{p(d_k|D)}{p_g(d_k)} = \\frac{p(g(D, U_k)|D)}{p_g(g(D, U_k))}$$\n",
    "\n",
    "These importance weights correct for the discrepancy between the distribution of synthetic datasets $p_g(D')$ and the true posterior distribution over datasets given the original data, $p(D'|D)$.  This posterior is generally intractable, which makes calculating these weights directly difficult.\n",
    "\n",
    "**Approximations and Practical Considerations:**\n",
    "\n",
    "Since computing $p(d_k|D)$ and $p_g(d_k)$ is usually intractable, we need to consider approximations. Here are some possible directions:\n",
    "\n",
    "* **Assume $p(D'|D) \\approx p_g(D')$:**  If we assume that the distribution of synthetic datasets is a reasonable approximation to the true posterior over datasets, then the weights can be simplified to $\\omega_k \\approx 1/m$, which corresponds to a simple average.  This is the simplest approach but might not be accurate if $g$ introduces significant biases.\n",
    "\n",
    "* **Density Estimation:**  We could try to estimate $p_g(d_k)$ using density estimation techniques. This is generally challenging in high dimensions.\n",
    "\n",
    "* **Variational Inference:**  We could try to learn a variational approximation to $p(D'|D)$ and use this to compute the importance weights.\n",
    "\n",
    "* **Metropolis-Hastings:**  We could potentially adapt Metropolis-Hastings to sample from $p(D'|D)$ directly, although this might be computationally expensive.\n",
    "\n",
    "The choice of approximation will depend on the specific form of $g$ and the complexity of the data.  Understanding the properties of $g$ is crucial for choosing an appropriate weighting scheme and assessing the validity of the resulting uncertainty estimates.  Your intuition about the behavior of $g$ for different values of $u$ is a good starting point for designing more informed weighting strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is my procedure to approximate the uncertainty of the output of a NN conditioned on the training data $d$:\n",
    "1. generate samples $u_k$, $k=1 \\dots m$ ($u \\in [0,1]$)\n",
    "2. generate synthetic datasets $d_k \\coloneqq g(d, u_k)$\n",
    "3. train NNs $\\hat{f}_k$ on those\n",
    "4. evaluate those: $\\hat{y}_k(x) = \\hat{f}_k(x)$\n",
    "5. weight samples such that correct distribution is obtained\n",
    "\n",
    "I am not sure about the weighting process, which is why I come to you. I just know that I have to reweight because of the following thought experiment:\n",
    "I know that $g$ gives synthetic datasets that can look completely different from another for large, but different $u$'s. The opposite is true for small but different $u$'s.\n",
    "Imagine two different sampling strategies for $u_k$. The first gives very large $u$'s and the second very small ones. Since my NN learns these datasets, the same while hold for the predicted values $\\hat{y}_k(x)$: First samling strategy -> high variance. Second samling strategy -> low variance. So two different uncertainty quantifications for the same dataset $d$.\n",
    "\n",
    "Formalize my procedure step by step using exact mathematical notation.\n",
    "\n",
    "Make sure to use consistent notation: Uppercase for random variables, f,g,h,... for functions and so on. Be extra precise and pedantic about everything.\n",
    "\n",
    "So $D$ is the dataset RV. $d$ is a realization of it.\n",
    "\n",
    "And I am looking for $p(\\hat{Y}(x) | D)$, where $\\hat{Y}(x)$ is the output of a NN (also a RV because of the dataset samples) for a $x$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
